<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>The Trajectory — January 15, 2026</title>

    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Fraunces:opsz,wght@9..144,400;9..144,600;9..144,700&family=Source+Sans+3:wght@400;500;600&family=JetBrains+Mono:wght@400;500&display=swap" rel="stylesheet">

    <style>
        :root {
            --bg: #FDFBF7;
            --bg-alt: #F0F5F8;
            --text: #1A1815;
            --text-secondary: #4D5C6A;
            --accent: #0D6E8A;
            --accent-soft: rgba(13, 110, 138, 0.12);
            --accent-hover: #095570;
            --border: #D8E2E8;
            --border-strong: #B8CCD6;

            --font-display: 'Fraunces', Georgia, serif;
            --font-body: 'Source Sans 3', -apple-system, sans-serif;
            --font-mono: 'JetBrains Mono', monospace;

            --space-xs: 0.5rem;
            --space-sm: 1rem;
            --space-md: 1.5rem;
            --space-lg: 2.5rem;
            --space-xl: 4rem;
            --space-2xl: 6rem;

            --content-width: 680px;
        }

        @media (prefers-color-scheme: dark) {
            :root {
                --bg: #0F1318;
                --bg-alt: #161C22;
                --text: #F5F1EA;
                --text-secondary: #9AACB8;
                --accent: #3DBDE8;
                --accent-soft: rgba(61, 189, 232, 0.15);
                --accent-hover: #5CCDF5;
                --border: #252E38;
                --border-strong: #354050;
            }
        }

        *, *::before, *::after { box-sizing: border-box; }

        html {
            font-size: 18px;
            scroll-behavior: smooth;
        }

        body {
            margin: 0;
            padding: 0;
            font-family: var(--font-body);
            font-weight: 400;
            line-height: 1.7;
            color: var(--text);
            background: var(--bg);
            -webkit-font-smoothing: antialiased;
        }

        .header {
            padding: var(--space-lg) var(--space-md);
            border-bottom: 1px solid var(--border);
        }

        .header-inner {
            max-width: var(--content-width);
            margin: 0 auto;
            display: flex;
            justify-content: space-between;
            align-items: baseline;
        }

        .newsletter-brand {
            font-family: var(--font-display);
            font-size: 1.1rem;
            font-weight: 600;
            color: var(--text);
            text-decoration: none;
        }

        .newsletter-date {
            font-family: var(--font-mono);
            font-size: 0.75rem;
            color: var(--text-secondary);
        }

        .hero {
            padding: var(--space-xl) var(--space-md) var(--space-2xl);
            text-align: center;
        }

        .hero-inner {
            max-width: var(--content-width);
            margin: 0 auto;
        }

        .hero-label {
            display: inline-block;
            font-family: var(--font-mono);
            font-size: 0.7rem;
            font-weight: 500;
            text-transform: uppercase;
            letter-spacing: 0.15em;
            color: var(--accent);
            background: var(--accent-soft);
            padding: 0.4em 1em;
            border-radius: 2px;
            margin-bottom: var(--space-md);
        }

        .hero-title {
            font-family: var(--font-display);
            font-size: clamp(2.2rem, 5vw, 3.2rem);
            font-weight: 700;
            line-height: 1.15;
            letter-spacing: -0.025em;
            margin: 0 0 var(--space-md);
            color: var(--text);
        }

        .hero-subtitle {
            font-size: 1.15rem;
            color: var(--text-secondary);
            max-width: 540px;
            margin: 0 auto;
            line-height: 1.6;
        }

        .hero-image {
            margin-top: var(--space-xl);
            border-radius: 8px;
            overflow: hidden;
            box-shadow: 0 4px 6px rgba(0,0,0,0.04), 0 12px 24px rgba(0,0,0,0.06);
        }

        .hero-image img {
            width: 100%;
            height: auto;
            display: block;
        }

        .content {
            max-width: var(--content-width);
            margin: 0 auto;
            padding: 0 var(--space-md);
        }

        .section {
            padding: var(--space-xl) 0;
            border-bottom: 1px solid var(--border);
            opacity: 0;
            transform: translateY(20px);
            animation: fadeInUp 0.6s ease forwards;
        }

        .section:nth-child(1) { animation-delay: 0.1s; }
        .section:nth-child(2) { animation-delay: 0.2s; }
        .section:nth-child(3) { animation-delay: 0.3s; }
        .section:nth-child(4) { animation-delay: 0.4s; }
        .section:nth-child(5) { animation-delay: 0.5s; }

        .section:last-child { border-bottom: none; }

        @keyframes fadeInUp {
            to { opacity: 1; transform: translateY(0); }
        }

        .section-number {
            font-family: var(--font-mono);
            font-size: 0.7rem;
            font-weight: 500;
            color: var(--accent);
            letter-spacing: 0.1em;
            margin-bottom: var(--space-xs);
        }

        .section-title {
            font-family: var(--font-display);
            font-size: 1.6rem;
            font-weight: 600;
            line-height: 1.3;
            letter-spacing: -0.015em;
            margin: 0 0 var(--space-sm);
            color: var(--text);
        }

        .section-meta {
            display: flex;
            gap: var(--space-sm);
            align-items: center;
            margin-bottom: var(--space-md);
            flex-wrap: wrap;
        }

        .section-source {
            font-family: var(--font-mono);
            font-size: 0.72rem;
            color: var(--text-secondary);
        }

        .section-source a {
            color: var(--accent);
            text-decoration: none;
            border-bottom: 1px solid transparent;
            transition: border-color 0.2s ease;
        }

        .section-source a:hover { border-color: var(--accent); }

        .section-date {
            font-family: var(--font-mono);
            font-size: 0.72rem;
            color: var(--text-secondary);
        }

        .section-date::before {
            content: "•";
            margin: 0 0.5em;
        }

        .section-body p {
            margin: 0 0 var(--space-sm);
        }

        .section-body p:last-child { margin-bottom: 0; }

        .section-body a {
            color: var(--accent);
            text-decoration: underline;
            text-decoration-color: var(--accent-soft);
            text-underline-offset: 2px;
        }

        .section-body a:hover { text-decoration-color: var(--accent); }

        .callout {
            background: var(--bg-alt);
            border-left: 3px solid var(--accent);
            padding: var(--space-md);
            margin: var(--space-md) 0;
            border-radius: 0 4px 4px 0;
        }

        .callout p {
            margin: 0;
            font-size: 0.95rem;
        }

        /* Charts and data visualizations */
        .chart {
            margin: var(--space-md) 0;
            padding: 0;
        }

        .chart img {
            width: 100%;
            height: auto;
            display: block;
            border-radius: 6px;
            box-shadow: 0 2px 8px rgba(0,0,0,0.06);
        }

        .chart figcaption {
            font-family: var(--font-mono);
            font-size: 0.75rem;
            color: var(--text-secondary);
            margin-top: var(--space-xs);
            padding: 0 var(--space-xs);
            line-height: 1.5;
        }

        code {
            font-family: var(--font-mono);
            font-size: 0.85em;
            background: var(--bg-alt);
            padding: 0.15em 0.4em;
            border-radius: 3px;
            color: var(--accent);
        }

        .closing {
            padding: var(--space-2xl) var(--space-md);
            text-align: center;
            background: var(--bg-alt);
        }

        .closing-inner {
            max-width: var(--content-width);
            margin: 0 auto;
        }

        .closing-title {
            font-family: var(--font-display);
            font-size: 1.4rem;
            font-weight: 600;
            margin: 0 0 var(--space-sm);
        }

        .closing-text {
            color: var(--text-secondary);
            margin: 0;
        }

        .footer {
            padding: var(--space-lg) var(--space-md);
            border-top: 1px solid var(--border);
        }

        .footer-inner {
            max-width: var(--content-width);
            margin: 0 auto;
            display: flex;
            justify-content: center;
        }

        .footer-brand {
            font-family: var(--font-display);
            font-size: 0.9rem;
            font-weight: 600;
            color: var(--text);
        }

        @media (max-width: 600px) {
            html { font-size: 16px; }
            .header-inner, .footer-inner {
                flex-direction: column;
                align-items: flex-start;
                gap: var(--space-xs);
            }
            .hero { padding: var(--space-lg) var(--space-md) var(--space-xl); }
            .hero-title { font-size: 1.9rem; }
            .section { padding: var(--space-lg) 0; }
            .section-title { font-size: 1.35rem; }
        }
    </style>
</head>
<body>
    <header class="header">
        <div class="header-inner">
            <a href="index.html" class="newsletter-brand">JLW Newsletter</a>
            <time class="newsletter-date">January 15, 2026</time>
        </div>
    </header>

    <section class="hero">
        <div class="hero-inner">
            <span class="hero-label">AI Forecasting</span>
            <h1 class="hero-title">The Trajectory</h1>
            <p class="hero-subtitle">Where will LLMs be in one year? Three years? The research community is placing bets—and the predictions range from "AGI by 2027" to "we're already hitting walls." Here's what the data actually says.</p>

            <div class="hero-image">
                <img src="images/hero-trajectory-v2.jpg" alt="Abstract visualization of exponential growth curves and AI capability trajectories">
            </div>
        </div>
    </section>

    <main class="content">
        <!-- SECTION 1: Dec 31, 2025 - Understanding AI -->
        <article class="section">
            <span class="section-number">01</span>
            <h2 class="section-title">17 Experts Bet on 2026: Half a Work Week of AI Tasks, No Economic Miracle</h2>
            <div class="section-meta">
                <span class="section-source">via <a href="https://www.understandingai.org/p/17-predictions-for-ai-in-2026">Understanding AI</a></span>
                <span class="section-date">Dec 31, 2025</span>
            </div>
            <div class="section-body">
                <p><a href="https://www.understandingai.org/">Understanding AI</a> assembled nine expert contributors to make 17 concrete predictions for 2026. The consensus: continued capability gains, but nothing that transforms the economy overnight.</p>

                <p><a href="https://twitter.com/baboratory">Timothy B. Lee</a> predicts Big Tech capex will exceed $500 billion (75% confidence), with <a href="https://openai.com">OpenAI</a> hitting $30 billion revenue and <a href="https://anthropic.com">Anthropic</a> reaching $15 billion. <a href="https://twitter.com/kaiwaitwhat">Kai Williams</a> forecasts AI completing 20-hour software tasks by year-end—half a human work week—at 55% confidence. But Lee directly challenges "fast takeoff" theories: he gives 90% odds that US GDP growth stays below 3.5%, far from the explosive growth some predict.</p>

                <p>Other notable bets: context windows plateau around one million tokens (80%), a Chinese company surpasses <a href="https://waymo.com">Waymo</a> in fleet size (55%), and <a href="https://tesla.com">Tesla</a> launches truly driverless taxis (70%). <a href="https://twitter.com/grimmelm">James Grimmelmann</a> predicts the legal free-for-all ends, with courts imposing serious financial consequences on AI companies.</p>

                <div class="callout">
                    <p><strong>The meta-lesson from 2025:</strong> Any advantage for an AI lab was temporary. Once one lab proved a capability, others quickly followed. The contributors expect this "fast-follower" dynamic to persist—meaning no one stays ahead for long.</p>
                </div>
            </div>
        </article>

        <!-- SECTION 2: Dec 30, 2025 - Sebastian Raschka -->
        <article class="section">
            <span class="section-number">02</span>
            <h2 class="section-title">Raschka's 2025 Retrospective: Inference Scaling Is the New Frontier</h2>
            <div class="section-meta">
                <span class="section-source">via <a href="https://magazine.sebastianraschka.com/p/state-of-llms-2025">Ahead of AI</a></span>
                <span class="section-date">Dec 30, 2025</span>
            </div>
            <div class="section-body">
                <p><a href="https://sebastianraschka.com/">Sebastian Raschka</a>, author of "Build A Large Language Model (From Scratch)" and researcher with 150,000+ subscribers, published his annual state-of-the-field analysis. The headline: the industry pivoted from training-centric to inference-centric optimization.</p>

                <p>The year's watershed moment was <a href="https://deepseek.com">DeepSeek</a> R1's January release. Their transparency revealed striking economics: training their reasoning model cost just $294,000—dramatically lower than the hundreds of millions industry assumed. Reinforcement Learning with Verifiable Rewards (RLVR) and the GRPO algorithm fundamentally changed post-training, enabling reasoning capabilities without massive compute.</p>

                <p>Raschka identifies "benchmaxxing" as a growing crisis: public test contamination means leaderboard scores no longer reliably predict real-world performance. His 2026 predictions center on inference-time gains, diffusion models for consumer applications, and RLVR expanding beyond math and coding into chemistry and biology. Classical RAG, he argues, will fade as long-context windows improve.</p>

                <div class="callout">
                    <p><strong>The quiet shift:</strong> <a href="https://qwen.ai">Qwen</a> displaced <a href="https://llama.meta.com">Meta's Llama</a> as the open-source community standard in 2025. Private data emerged as the key competitive moat—companies increasingly refuse to license proprietary datasets to AI labs.</p>
                </div>
            </div>
        </article>

        <!-- SECTION 3: Apr 2025 - AI 2027 -->
        <article class="section">
            <span class="section-number">03</span>
            <h2 class="section-title">The AI 2027 Scenario: AGI in Two Years, Superintelligence in Three</h2>
            <div class="section-meta">
                <span class="section-source">via <a href="https://ai-2027.com/">AI 2027</a></span>
                <span class="section-date">Apr 2025</span>
            </div>
            <div class="section-body">
                <p>In April 2025, <a href="https://www.linkedin.com/in/daniel-kokotajlo/">Daniel Kokotajlo</a>—who correctly predicted the trends leading to <a href="https://chat.openai.com">ChatGPT</a>, GPT-4o, and o1 nearly four years ago—published <a href="https://ai-2027.com/">AI 2027</a>, a detailed scenario for rapid AI progress. The prediction: AGI by 2027, superintelligence by 2028.</p>

                <p>The scenario draws on <a href="https://situational-awareness.ai/">Leopold Aschenbrenner's</a> 165-page "Situational Awareness" treatise from 2024, which projected AGI through continued exponential scaling. AI 2027 maps it concretely: a fictional "Agent-1" evolves through increasingly capable versions until, by September 2027, Agent-4 achieves superhuman AI research capabilities—making a year's worth of breakthroughs every week. Humans become spectators.</p>

                <p>The authors added a November 2025 note: "2027 was our modal (most likely) year at publication, our medians were somewhat longer." A predictions tracker shows 91% accuracy on evaluated forecasts so far—though only 18% have been assessed. Critics like <a href="https://garymarcus.substack.com/">Gary Marcus</a> remain skeptical: "even though I highly doubt AGI will arrive in three years, I can't absolutely promise you it won't happen in 10."</p>

                <div class="callout">
                    <p><strong>The geopolitical dimension:</strong> Aschenbrenner frames AGI as an intensifying US-China race requiring "Manhattan Project"-style initiatives. By late 2026, the scenario predicts an AI arms race in full swing, with both nations making dramatic moves to outpace each other.</p>
                </div>
            </div>
        </article>

        <!-- SECTION 4: Mar 2025 - METR -->
        <article class="section">
            <span class="section-number">04</span>
            <h2 class="section-title">METR's Doubling Law: AI Task Duration Doubles Every 7 Months</h2>
            <div class="section-meta">
                <span class="section-source">via <a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">METR</a></span>
                <span class="section-date">Mar 2025</span>
            </div>
            <div class="section-body">
                <p><a href="https://metr.org/">METR</a> (Model Evaluation and Threat Research) proposed a new way to measure AI progress: the length of tasks AI can complete autonomously. Their finding is striking: this metric has doubled approximately every 7 months for the past six years, possibly accelerating to every 4 months in 2024.</p>

                <figure class="chart">
                    <img src="images/chart-task-duration.png" alt="Line chart showing AI task duration doubling every 7 months from 2019-2025">
                    <figcaption>AI-completable task duration has grown exponentially, from ~1 minute in 2019 to ~1 hour in 2025. Source: METR (Mar 2025)</figcaption>
                </figure>

                <p>The methodology: 170 tasks across software engineering, cybersecurity, and reasoning, with human completion times ranging from seconds to tens of hours. Over 800 human baselines from skilled professionals established typical durations. The best current models—like <a href="https://claude.ai">Claude 3.7 Sonnet</a>—can handle some tasks that take expert humans hours, but reliably complete only tasks of a few minutes.</p>

                <p>Extrapolating the trend: in under a decade, AI agents could independently complete tasks that currently take humans days or weeks. By late 2026, the forecast suggests 50% reliability on 20-hour software tasks—roughly half a work week.</p>

                <div class="callout">
                    <p><strong>The productivity paradox:</strong> METR also ran a randomized controlled trial on real-world productivity. The surprising result: when experienced open-source developers used AI tools on their own repositories, they took 19% longer than without AI. Benchmark capability and practical utility don't always align.</p>
                </div>
            </div>
        </article>

        <!-- SECTION 5: Jun 2024 - Epoch AI -->
        <article class="section">
            <span class="section-number">05</span>
            <h2 class="section-title">Epoch AI's Data Wall: Training Data Exhaustion Between 2026 and 2032</h2>
            <div class="section-meta">
                <span class="section-source">via <a href="https://epoch.ai/blog/will-we-run-out-of-data-limits-of-llm-scaling-based-on-human-generated-data">Epoch AI</a></span>
                <span class="section-date">Jun 2024</span>
            </div>
            <div class="section-body">
                <p><a href="https://epoch.ai/">Epoch AI</a> researchers <a href="https://www.linkedin.com/in/pablo-villalobos-ai/">Pablo Villalobos</a>, <a href="https://www.linkedin.com/in/jaimesevilla/">Jaime Sevilla</a>, and collaborators tackled a fundamental constraint: how much training data actually exists? Their estimate: approximately 300 trillion tokens of quality, repetition-adjusted human-generated public text.</p>

                <figure class="chart">
                    <img src="images/chart-data-exhaustion.png" alt="Chart showing training data consumption approaching the data ceiling between 2026-2028">
                    <figcaption>Training data consumption is accelerating toward available supply. The "data wall" arrives between 2026-2028 depending on overtraining intensity. Source: Epoch AI (2024-2025)</figcaption>
                </figure>

                <p>The exhaustion timeline depends on training intensity. With 5x overtraining (training on data multiple times), models hit the wall around 2027. Without overtraining, around 2028. With aggressive 100x overtraining, potentially 2025. The breakdown: CommonCrawl offers ~130 trillion tokens, the indexed web ~510 trillion, and the whole web including private content ~3,100 trillion.</p>

                <p>The researchers acknowledge progress need not halt at data exhaustion. Synthetic data generation, multi-modal learning, and efficiency improvements offer paths forward. But the core constraint remains: human-generated text is finite, and current consumption patterns are depleting it rapidly.</p>

                <div class="callout">
                    <p><strong>The semiconductor bottleneck:</strong> Beyond data, <a href="https://www.tsmc.com">TSMC</a> is fully booked until 2026. Building new wafer fabs involves long lead times and equipment shortages. The exponential increase in chip deployment is pushing manufacturing toward its production ceiling—another potential wall.</p>
                </div>
            </div>
        </article>
    </main>

    <section class="closing">
        <div class="closing-inner">
            <h2 class="closing-title">The Three-Year Window</h2>
            <p class="closing-text">One year out: AI likely completes half-workweek tasks, inference optimization delivers most gains, and the legal reckoning begins. Three years out: the forecasts diverge wildly. The optimists see AGI and an intelligence explosion. The skeptics see data exhaustion, semiconductor bottlenecks, and diminishing returns forcing a rethink. Both camps agree on one thing: the trajectory we're on—exponential capability gains through pure scaling—is approaching its limits. What comes next depends on whether we find new curves to climb.</p>
        </div>
    </section>

    <footer class="footer">
        <div class="footer-inner">
            <span class="footer-brand">JLW Newsletter</span>
        </div>
    </footer>
</body>
</html>
